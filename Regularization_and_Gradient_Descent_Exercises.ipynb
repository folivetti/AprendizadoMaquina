{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularização e Gradiente Descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Vamos iniciar essa lista com um pequeno tutorial sobre regressão, atributos polinomiais e regularização em uma base de dados bem simples contendo uma coluna de variáveis `x` e um valor `y` associado. A base de dados é chamada de `X_Y_Sinusoid_Data.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#Create data folder and copy  \"X_Y_Sinusoid_Data.csv\" and \"Ames_Housing_Sales.csv\"\n",
    "#data_path = ['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Importe a base de dados. \n",
    "* Gere aproximadamente  100 pontos x na faixa de 0 a 1. Usando esses pontos, calcule o ponto y representando o valor verdadeiro a partir da equação: $y = sin(2\\pi x)$\n",
    "\n",
    "* Plote a base e `x` vs `y` da base e o gerado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#filepath = os.sep.join(data_path + ['X_Y_Sinusoid_Data.csv'])\n",
    "#data = pd.read_csv(filepath)\n",
    "\n",
    "#X_real = np.linspace(0, 1.0, 100)\n",
    "#Y_real = np.sin(2 * np.pi * X_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:32:58.021116Z",
     "start_time": "2017-03-10T14:32:58.015025-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:32:58.921638Z",
     "start_time": "2017-03-10T14:32:58.668630-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.set_style('white')\n",
    "#sns.set_context('talk')\n",
    "#sns.set_palette('dark')\n",
    "\n",
    "# Plot of the noisy (sparse)\n",
    "#ax = data.set_index('x')['y'].plot(ls='', marker='o', label='data')\n",
    "#ax.plot(X_real, Y_real, ls='--', marker='', label='real function')\n",
    "\n",
    "#ax.legend()\n",
    "#ax.set(xlabel='x data', ylabel='y data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "\n",
    "* Usando a classe `PolynomialFeatures` da biblioteca de pré-processamento do Scikit-learn's, crie atributos polinomiais de ordem 20.\n",
    "* Crie um modelo de regressão linear. \n",
    "* Plote o valor predito com o valor calculado.\n",
    "\n",
    "Note que `PolynomialFeatures` requer um dataframe com 1 coluna ou uma array bidimensional de dimensão (`n`, 1), com `n` sendo o número de amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T19:57:23.344020Z",
     "start_time": "2017-03-10T14:57:23.057905-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Setup the polynomial features\n",
    "degree = 20\n",
    "pf = PolynomialFeatures(degree)\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Repita o experimento anterior utilizando ridge regression ($\\alpha$=0.001) e lasso regression ($\\alpha$=0.0001). \n",
    "* Plote os resultados.\n",
    "* Also plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients will likely need a separate plot (or their own y-axis) due to their large magnitude. \n",
    "\n",
    "What does the comparatively large magnitude of the data tell you about the role of regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T20:11:47.526408Z",
     "start_time": "2017-03-10T15:11:47.216623-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mute the sklearn warning about regularization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's look at the absolute value of coefficients for each model\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "#update coefficients with all models\n",
    "\n",
    "#describe coefficients\n",
    "# Huge difference in scale between non-regularized vs regularized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T20:39:37.722464Z",
     "start_time": "2017-03-10T15:39:37.347911-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#colors = sns.color_palette()\n",
    "\n",
    "# Setup the dual y-axes\n",
    "#ax1 = plt.axes()\n",
    "#ax2 = ax1.twinx()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "For the remaining questions, we will be working with the [data set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from last lesson, which is based on housing prices in Ames, Iowa. There are an extensive number of features--see the exercises from week three for a discussion of these features.\n",
    "\n",
    "To begin:\n",
    "\n",
    "* Import the data with Pandas, remove any null values, and one hot encode categoricals. Either Scikit-learn's feature encoders or Pandas `get_dummies` method can be used.\n",
    "* Split the data into train and test sets. \n",
    "* Log transform skewed features. \n",
    "* Scaling can be attempted, although it can be interesting to see how well regularization works without scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:04.174800Z",
     "start_time": "2017-03-10T12:01:04.142735-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\n",
    "#data1 = pd.read_csv(filepath, sep=',')\n",
    "#data = data1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T02:40:49.956043Z",
     "start_time": "2017-03-09T21:40:49.950878-05:00"
    }
   },
   "source": [
    "Create a list of categorial data and one-hot encode. Pandas one-hot encoder (`get_dummies`) works well with data that is defined as a categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:05.304547Z",
     "start_time": "2017-03-10T12:01:05.231567-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split the data in train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:06.260979Z",
     "start_time": "2017-03-10T12:01:06.244259-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of columns that have skewed features--a log transformation can be applied to them. Note that this includes the `SalePrice`, our predictor. However, let's keep that one as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of float colums to check for skewing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:10.689590Z",
     "start_time": "2017-03-10T12:01:10.609841-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all the columns where the skew is greater than 0.75, excluding \"SalePrice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:15.654621Z",
     "start_time": "2017-03-10T12:01:13.780771-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mute the setting wtih a copy warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate features from predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T17:01:21.972625Z",
     "start_time": "2017-03-10T12:01:21.957050-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:11:03.256453",
     "start_time": "2017-02-21T09:11:03.241117"
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "* Write a function **`rmse`** that takes in truth and prediction values and returns the root-mean-squared error. Use sklearn's `mean_squared_error`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit a basic linear regression model\n",
    "* print the root-mean-squared error for this model\n",
    "* plot the predicted vs actual sale price based on the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Ridge regression uses L2 normalization to reduce the magnitude of the coefficients. This can be helpful in situations where there is high variance. The regularization functions in Scikit-learn each contain versions that have cross-validation built in.\n",
    "\n",
    "* Fit a regular (non-cross validated) Ridge model to a range of $\\alpha$ values and plot the RMSE using the cross validated error function you created above.\n",
    "* Use $$[0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]$$ as the range of alphas.\n",
    "* Then repeat the fitting of the Ridge models using the range of $\\alpha$ values from the prior section. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the `RidgeCV` method. It's not possible to get the alpha values for the models that weren't selected, unfortunately. The resulting error values and $\\alpha$ values are very similar to those obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:48:27.914740",
     "start_time": "2017-02-21T09:48:27.293957"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "\n",
    "Much like the `RidgeCV` function, there is also a `LassoCV` function that uses an L1 regularization function and cross-validation. L1 regularization will selectively shrink some coefficients, effectively performing feature elimination.\n",
    "\n",
    "The `LassoCV` function does not allow the scoring function to be set. However, the custom error function (`rmse`) created above can be used to evaluate the error on the final model.\n",
    "\n",
    "Similarly, there is also an elastic net function with cross validation, `ElasticNetCV`, which is a combination of L2 and L1 regularization.\n",
    "\n",
    "* Fit a Lasso model using cross validation and determine the optimum value for $\\alpha$ and the RMSE using the function created above. Note that the magnitude of $\\alpha$ may be different from the Ridge model.\n",
    "* Repeat this with the Elastic net model.\n",
    "* Compare the results via table and/or plot.\n",
    "\n",
    "Use the following alphas:  \n",
    "`[1e-5, 5e-5, 0.0001, 0.0005]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:50:10.797247",
     "start_time": "2017-02-21T09:50:09.006978"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine how many of these features remain non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:50:13.053851",
     "start_time": "2017-02-21T09:50:13.047466"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T12:03:06.013488",
     "start_time": "2017-02-16T12:03:06.007159"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Now try the elastic net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:51:07.592747",
     "start_time": "2017-02-21T09:50:38.683133"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the RMSE calculation from all models is easiest in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:52:29.062678",
     "start_time": "2017-02-21T09:52:28.998572"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a plot of actual vs predicted housing prices as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T09:53:54.142116",
     "start_time": "2017-02-21T09:53:53.857081"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Let's explore Stochastic gradient descent in this exercise.  \n",
    "Recall that Linear models in general are sensitive to scaling.\n",
    "However, SGD is *very* sensitive to scaling.  \n",
    "Moreover, a high value of learning rate can cause the algorithm to diverge, whereas a too low value may take too long to converge.\n",
    "\n",
    "* Fit a stochastic gradient descent model without a regularization penalty (the relevant parameter is `penalty`).\n",
    "* Now fit stochastic gradient descent models with each of the three penalties (L2, L1, Elastic Net) using the parameter values determined by cross validation above. \n",
    "* Do not scale the data before fitting the model.  \n",
    "* Compare the results to those obtained without using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SGDRegressor and prepare the parameters\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how high the error values are! The algorithm is diverging. This can be due to scaling and/or learning rate being too high. Let's adjust the learning rate and see what happens.\n",
    "\n",
    "* Pass in `eta0=1e-7` when creating the instance of `SGDClassifier`.\n",
    "* Re-compute the errors for all the penalties and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SGDRegressor and prepare the parameters\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale our training data and try again.\n",
    "\n",
    "* Fit a `MinMaxScaler` to `X_train` create a variable `X_train_scaled`.\n",
    "* Using the scaler, transform `X_test` and create a variable `X_test_scaled`. \n",
    "* Apply the same versions of SGD to them and compare the results. Don't pass in a eta0 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
